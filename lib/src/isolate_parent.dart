import 'dart:async';
import 'dart:typed_data';
import 'package:llama_cpp_dart/src/llama_input.dart';
import 'package:typed_isolate/typed_isolate.dart';

import 'isolate_child.dart';
import 'isolate_scope.dart';
import 'isolate_types.dart';
import 'llama.dart';
import 'prompt_format.dart';

/// Event emitted when a completion finishes or fails
class CompletionEvent {
  final String promptId;
  final bool success;
  final String? errorDetails;
  CompletionEvent(this.promptId, this.success, [this.errorDetails]);
}

/// Structure to hold queued prompts
class _QueuedPrompt {
  final String prompt;
  final Completer<String> idCompleter = Completer<String>();
  final Object? scope;
  final List<LlamaImage>? images;
  _QueuedPrompt(this.prompt, this.scope, {this.images});
}

/// Parent class that manages communication with the LlamaChild isolate
class LlamaParent {
  final StreamController<String> _controller = StreamController<String>.broadcast();
  final _parent = IsolateParent<LlamaCommand, LlamaResponse>();

  StreamSubscription<LlamaResponse>? _subscription;
  bool _isGenerating = false;
  LlamaStatus _status = LlamaStatus.uninitialized;
  LlamaStatus get status => _status;

  /// List of messages for chat history used by the formatter
  List<Map<String, dynamic>> messages = [];

  final LlamaLoad loadCommand;
  final PromptFormat? formatter;

  Completer<void>? _readyCompleter;
  Completer<void>? _operationCompleter;
  Completer<List<double>>? _embeddingsCompleter;

  Completer<Uint8List>? _stateCompleter;

  /// Maps prompt IDs to completers for operation tracking
  final Map<String, Completer<void>> _promptCompleters = {};

  /// Stream of text generated by the model
  Stream<String> get stream => _controller.stream;

  /// Whether text generation is in progress
  bool get isGenerating => _isGenerating;

  final _completionController = StreamController<CompletionEvent>.broadcast();
  Stream<CompletionEvent> get completions => _completionController.stream;

  String _currentPromptId = "";
  final List<dynamic> _scopes = [];
  final List<_QueuedPrompt> _promptQueue = [];
  bool _isProcessingQueue = false;

  Object? _currentScope;

  LlamaParent(this.loadCommand, [this.formatter]);

  Future<void> init() async {
    _readyCompleter = Completer<void>();
    _isGenerating = false;
    _status = LlamaStatus.uninitialized;

    _parent.init();
    await _subscription?.cancel();
    _subscription = _parent.stream.listen(_onData);

    await _parent.spawn(LlamaChild());

    await _sendCommand(LlamaInit(Llama.libraryPath), "library initialization");

    _status = LlamaStatus.loading;
    await _sendCommand(loadCommand, "model loading");

    await _readyCompleter!.future;
  }

  void _onData(LlamaResponse data) {
    if (data.stateData != null) {
      if (_stateCompleter != null && !_stateCompleter!.isCompleted) {
        _stateCompleter!.complete(data.stateData);
        _stateCompleter = null;
      }
      return; 
    }

    if (data.embeddings != null) {
      if (_embeddingsCompleter != null && !_embeddingsCompleter!.isCompleted) {
        _embeddingsCompleter!.complete(data.embeddings);
        _embeddingsCompleter = null;
      }
      return;
    }

    if (data.status == LlamaStatus.error && data.errorDetails != null) {
      final ex = LlamaException(data.errorDetails!);
      
      if (_operationCompleter != null && !_operationCompleter!.isCompleted) {
        _operationCompleter!.completeError(ex);
        _operationCompleter = null;
      }
      if (_stateCompleter != null && !_stateCompleter!.isCompleted) {
        _stateCompleter!.completeError(ex);
        _stateCompleter = null;
      }
    }

    if (data.status != null) {
      _status = data.status!;
      if (data.status == LlamaStatus.ready &&
          _readyCompleter != null &&
          !_readyCompleter!.isCompleted) {
        _readyCompleter!.complete();
      }
    }

    if (data.isConfirmation) {
      if (_operationCompleter != null && !_operationCompleter!.isCompleted) {
        _operationCompleter!.complete();
      }
    }

    if (data.text.isNotEmpty) {
      _controller.add(data.text);
      for (final scope in _scopes) {
        scope.handleResponse(data);
      }
    }

    if (data.isDone && data.stateData == null && data.embeddings == null && !data.isConfirmation) {
      _isGenerating = false;
      final promptId = data.promptId ?? _currentPromptId;

      if (_promptCompleters.containsKey(promptId)) {
        if (!_promptCompleters[promptId]!.isCompleted) {
          _promptCompleters[promptId]!.complete();
        }
        _promptCompleters.remove(promptId);
      }

      final success = data.status != LlamaStatus.error;
      final event = CompletionEvent(promptId, success, data.errorDetails);

      _completionController.add(event);
      for (final scope in _scopes) {
        scope.handleCompletion(event);
      }
      _currentScope = null;
    }
  }

  /// Requests the child to save the VRAM state of a specific scope to RAM.
  Future<Uint8List> saveState(LlamaScope scope) async {
    _stateCompleter = Completer<Uint8List>();
    _parent.sendToChild(id: 1, data: LlamaSaveState(scope.id));
    
    return _stateCompleter!.future.timeout(
      const Duration(seconds: 30),
      onTimeout: () {
        _stateCompleter = null;
        throw TimeoutException("Save state timed out");
      }
    );
  }

  /// Requests the child to load RAM state into a VRAM slot.
  Future<void> loadState(LlamaScope scope, Uint8List data) async {
    await _sendCommand(LlamaLoadState(scope.id, data), "load state");
  }

  /// Requests the child to load a Disk session into a VRAM slot.
  Future<void> loadSession(LlamaScope scope, String path) async {
    await _sendCommand(LlamaLoadSession(scope.id, path), "load session");
  }

  /// Requests the child to free the C++ slot associated with this scope.
  Future<void> disposeScope(LlamaScope scope) async {
    // Ensure any pending or in-flight prompts for this scope are stopped before
    // releasing its native slot.
    await cancelScope(scope, cancelInFlight: true, cancelQueued: true);
    _scopes.remove(scope);
    await _sendCommand(LlamaFreeSlot(scope.id), "free slot");
  }

  Future<void> _sendCommand(LlamaCommand command, String description) async {
    _operationCompleter = Completer<void>();
    _parent.sendToChild(data: command, id: 1);

    return await _operationCompleter!.future.timeout(
      Duration(seconds: description == "model loading" ? 60 : 30),
      onTimeout: () {
        if (command is LlamaStop) {
           // ignore: avoid_print
           print("Warning: Stop command timed out, forcing state reset.");
           return; 
        }
        throw TimeoutException('Operation "$description" timed out');
      },
    );
  }

  Future<String> sendPrompt(String prompt, {Object? scope}) async {
    if (loadCommand.contextParams.embeddings) {
      throw StateError("Configured for embeddings only.");
    }
    final queuedPrompt = _QueuedPrompt(prompt, scope);
    _promptQueue.add(queuedPrompt);
    if (!_isProcessingQueue) _processNextPrompt();
    return queuedPrompt.idCompleter.future;
  }

  Future<String> sendPromptWithImages(String prompt, List<LlamaImage> images, {Object? scope}) async {
    if (loadCommand.contextParams.embeddings) {
      throw StateError("Configured for embeddings only.");
    }
    final queuedPrompt = _QueuedPrompt(prompt, scope, images: images);
    _promptQueue.add(queuedPrompt);
    if (!_isProcessingQueue) _processNextPrompt();
    return queuedPrompt.idCompleter.future;
  }

  Future<void> _processNextPrompt() async {
    if (_promptQueue.isEmpty) {
      _isProcessingQueue = false;
      return;
    }

    _isProcessingQueue = true;
    final nextPrompt = _promptQueue.removeAt(0);

    if (_isGenerating) {
      await stop();
      await Future.delayed(const Duration(milliseconds: 10));
    }

    _currentPromptId = DateTime.now().millisecondsSinceEpoch.toString();
    _promptCompleters[_currentPromptId] = Completer<void>();
    nextPrompt.idCompleter.complete(_currentPromptId);

    _currentScope = nextPrompt.scope;

    String? targetSlotId;
    if (nextPrompt.scope != null && nextPrompt.scope is LlamaScope) {
        targetSlotId = (nextPrompt.scope as LlamaScope).id;
        (nextPrompt.scope as LlamaScope).addPromptId(_currentPromptId);
    }

    final formattedPrompt = messages.isEmpty
        ? (formatter?.formatPrompt(nextPrompt.prompt) ?? nextPrompt.prompt)
        : (formatter?.formatMessages(messages) ?? nextPrompt.prompt);

    _isGenerating = true;
    _status = LlamaStatus.generating;

    _parent.sendToChild(
        id: 1,
        data: LlamaPrompt(formattedPrompt, _currentPromptId,
            images: nextPrompt.images, slotId: targetSlotId));

    _promptCompleters[_currentPromptId]!.future.whenComplete(() {
      _processNextPrompt();
    });
  }

  Future<void> waitForCompletion(String promptId) async {
    if (!_promptCompleters.containsKey(promptId)) return;
    final completer = _promptCompleters[promptId]!;
    await completer.future;
  }

  Future<void> stop() async {
    if (_isGenerating) {
      await _sendCommand(LlamaStop(), "generation stopping");
      _isGenerating = false;
    }
  }

  Future<List<double>> getEmbeddings(String prompt) async {
    if (!loadCommand.contextParams.embeddings) {
      throw StateError("Not configured for embeddings.");
    }
    _embeddingsCompleter = Completer();
    _parent.sendToChild(id: 1, data: LlamaEmbedd(prompt));
    return _embeddingsCompleter!.future;
  }

  Future<void> dispose() async {
    _isGenerating = false;
    _status = LlamaStatus.disposed;

    if (_readyCompleter != null && !_readyCompleter!.isCompleted) _readyCompleter!.completeError("Disposed");
    if (_operationCompleter != null && !_operationCompleter!.isCompleted) _operationCompleter!.completeError("Disposed");
    if (_embeddingsCompleter != null && !_embeddingsCompleter!.isCompleted) _embeddingsCompleter!.completeError("Disposed");
    if (_stateCompleter != null && !_stateCompleter!.isCompleted) _stateCompleter!.completeError("Disposed");

    for (var p in _promptQueue) {
      if (!p.idCompleter.isCompleted) p.idCompleter.completeError("Disposed");
    }
    _promptQueue.clear();

    for (final scope in List.from(_scopes)) {
      await scope.dispose();
    }
    _scopes.clear();

    if (!_controller.isClosed) await _controller.close();
    if (!_completionController.isClosed) await _completionController.close();

    _parent.sendToChild(id: 1, data: LlamaDispose());

    // Keep the subscription alive long enough to receive confirmations while
    // slots are being freed and the child is disposing.
    await Future.delayed(const Duration(milliseconds: 50));

    await _subscription?.cancel();

    _parent.dispose();
  }

  dynamic getScope() {
    final scope = LlamaScope(this);
    _scopes.add(scope);
    return scope;
  }

  Future<void> cancelScope(Object scope,
      {bool cancelInFlight = true, bool cancelQueued = true}) async {
    if (cancelQueued) {
      _promptQueue.removeWhere((p) {
        if (identical(p.scope, scope)) {
          if (!p.idCompleter.isCompleted) {
            p.idCompleter.completeError("Scope canceled");
          }
          return true;
        }
        return false;
      });
    }
    if (cancelInFlight && identical(_currentScope, scope) && _isGenerating) {
      await stop();
    }
  }
}
