import 'dart:async';
import 'package:llama_cpp_dart/llama_cpp_dart.dart';
import 'package:typed_isolate/typed_isolate.dart';

/// Event emitted when a completion finishes or fails
class CompletionEvent {
  final String promptId;
  final bool success;
  final String? errorDetails;

  CompletionEvent(this.promptId, this.success, [this.errorDetails]);
}

/// Structure to hold queued prompts
class _QueuedPrompt {
  final String prompt;
  final Completer<String> idCompleter = Completer<String>();
  final Object? scope;
  final List<LlamaImage>? images;

  _QueuedPrompt(this.prompt, this.scope, {this.images});
}

// Create a subclass for prompts with images (optional, for clarity):
class _QueuedPromptWithImages extends _QueuedPrompt {
  _QueuedPromptWithImages(super.prompt, List<LlamaImage> images, super.scope)
      : super(images: images);
}

/// Parent class that manages communication with the LlamaChild isolate
class LlamaParent {
  StreamController<String> _controller = StreamController<String>.broadcast();
  final _parent = IsolateParent<LlamaCommand, LlamaResponse>();

  StreamSubscription<LlamaResponse>? _subscription;
  bool _isGenerating = false;

  /// Status of the LlamaParent
  LlamaStatus _status = LlamaStatus.uninitialized;
  LlamaStatus get status => _status;

  /// List of messages for chat history
  List<Map<String, dynamic>> messages = [];

  final LlamaLoad loadCommand;
  final PromptFormat? formatter;

  Completer<void>? _readyCompleter;
  Completer<void>? _operationCompleter;

  /// Maps prompt IDs to completers for operation tracking
  final Map<String, Completer<void>> _promptCompleters = {};

  /// Stream of text generated by the model
  Stream<String> get stream => _controller.stream;

  /// Whether text generation is in progress
  bool get isGenerating => _isGenerating;

  /// Stream of completion events
  final _completionController = StreamController<CompletionEvent>.broadcast();
  Stream<CompletionEvent> get completions => _completionController.stream;

  /// Current prompt ID
  String _currentPromptId = "";

  /// List of active scopes
  final List<dynamic> _scopes = [];

  /// Queue for pending prompts
  final List<_QueuedPrompt> _promptQueue = [];
  bool _isProcessingQueue = false;

  /// Create a new LlamaParent
  ///
  /// [loadCommand] specifies the model to load
  /// [formatter] optional formatter for prompts
  LlamaParent(this.loadCommand, [this.formatter]);

  /// Handle responses from the child isolate
  void _onData(LlamaResponse data) {
    // Update status if provided
    if (data.status != null) {
      _status = data.status!;

      // Complete the ready completer if model is ready
      if (data.status == LlamaStatus.ready &&
          _readyCompleter != null &&
          !_readyCompleter!.isCompleted) {
        _readyCompleter!.complete();
      }
    }

    // Add text to the stream if there is any
    if (data.text.isNotEmpty) {
      _controller.add(data.text);

      // Also notify all scopes
      for (final scope in _scopes) {
        scope.handleResponse(data);
      }
    }

    // Handle confirmation messages
    if (data.isConfirmation) {
      if (_operationCompleter != null && !_operationCompleter!.isCompleted) {
        _operationCompleter!.complete();
      }
    }

    // Handle operation completion
    if (data.isDone) {
      _isGenerating = false;

      // Complete the prompt completer if it exists
      final promptId = data.promptId ?? _currentPromptId;
      if (_promptCompleters.containsKey(promptId)) {
        if (!_promptCompleters[promptId]!.isCompleted) {
          _promptCompleters[promptId]!.complete();
        }
        _promptCompleters.remove(promptId);
      }

      // Create completion event
      CompletionEvent event;
      if (data.status == LlamaStatus.error) {
        event = CompletionEvent(promptId, false, data.errorDetails);
      } else {
        event = CompletionEvent(promptId, true);
      }

      // Emit completion event to main stream
      _completionController.add(event);

      // Also notify scopes
      for (final scope in _scopes) {
        scope.handleCompletion(event);
      }
    }
  }

  /// Initialize the LlamaParent and load the model
  ///
  /// Returns a Future that completes when the model is ready
  Future<void> init() async {
    _readyCompleter = Completer<void>();

    _isGenerating = false;
    _status = LlamaStatus.uninitialized;
    _parent.init();

    // Cancel any existing subscription first
    await _subscription?.cancel();
    _subscription = _parent.stream.listen(_onData);

    // Spawn the child isolate
    await _parent.spawn(LlamaChild());

    // Initialize the library
    await _sendCommand(
        LlamaInit(Llama.libraryPath, loadCommand.modelParams,
            loadCommand.contextParams, loadCommand.samplingParams),
        "library initialization");

    // Load the model
    _status = LlamaStatus.loading;
    await _sendCommand(loadCommand, "model loading");

    // Wait for model to be ready
    await _readyCompleter!.future;
  }

  /// Send a command to the child isolate and wait for confirmation
  Future<void> _sendCommand(LlamaCommand command, String description) async {
    _operationCompleter = Completer<void>();

    _parent.sendToChild(data: command, id: 1);

    // Wait for confirmation with timeout
    return await _operationCompleter!.future.timeout(
      Duration(seconds: description == "model loading" ? 60 : 30),
      onTimeout: () {
        throw TimeoutException('Operation "$description" timed out');
      },
    );
  }

  /// Reset internal state for new generation
  Future<void> _reset() async {
    // If we're currently generating, stop it first
    if (_isGenerating) {
      await _stopGeneration();
    }

    // Recreate the stream controller if it was closed
    if (_controller.isClosed) {
      _controller = StreamController<String>.broadcast();

      // Reattach the subscription
      await _subscription?.cancel();
      _subscription = _parent.stream.listen(_onData);
    }

    // Clear the llama context
    await _sendCommand(LlamaClear(), "context clearing");

    // Status should be updated by confirmation from child
  }

  /// Stop any ongoing generation
  Future<void> _stopGeneration() async {
    if (_isGenerating) {
      await _sendCommand(LlamaStop(), "generation stopping");
      _isGenerating = false;

      // Status should be updated by confirmation from child
    }
  }

  /// Send a prompt to the model for text generation
  ///
  /// [prompt] The text prompt to generate from
  /// [scope] Optional scope to associate with this prompt
  ///
  /// Returns a prompt ID that can be used to track completion
  Future<String> sendPrompt(String prompt, {Object? scope}) async {
    if (loadCommand.contextParams.embeddings) {
      throw StateError(
          "This LlamaParent instance is configured for embeddings only and cannot generate text.");
    }

    // Create a queued prompt
    final queuedPrompt = _QueuedPrompt(prompt, scope);
    _promptQueue.add(queuedPrompt);

    // Start processing the queue if not already doing so
    if (!_isProcessingQueue) {
      _processNextPrompt();
    }

    // Return a Future that completes when this prompt gets an ID
    return queuedPrompt.idCompleter.future;
  }

  /// Process the next prompt in the queue
  Future<void> _processNextPrompt() async {
    if (_promptQueue.isEmpty) {
      _isProcessingQueue = false;
      return;
    }

    _isProcessingQueue = true;
    final nextPrompt = _promptQueue.removeAt(0);

    // Reset the model context
    await _reset();

    // Generate a unique ID for this prompt
    _currentPromptId = DateTime.now().millisecondsSinceEpoch.toString();

    // Create a completer for this prompt
    _promptCompleters[_currentPromptId] = Completer<void>();

    // Complete the ID completer so the caller gets the ID
    nextPrompt.idCompleter.complete(_currentPromptId);

    // Store the scope (if any) for this prompt ID
    if (nextPrompt.scope != null) {
      (nextPrompt.scope as dynamic).addPromptId(_currentPromptId);
    }

    // Format and send the prompt
    final formattedPrompt = messages.isEmpty
        ? (formatter?.formatPrompt(nextPrompt.prompt) ?? nextPrompt.prompt)
        : (formatter?.formatMessages(messages) ?? nextPrompt.prompt);

    // Mark that we're now generating and update status
    _isGenerating = true;
    _status = LlamaStatus.generating;

    // Send the prompt with images if available
    _parent.sendToChild(
        id: 1,
        data: LlamaPrompt(formattedPrompt, _currentPromptId,
            images: nextPrompt.images));

    // Set up completion handling for starting the next prompt
    _promptCompleters[_currentPromptId]!.future.then((_) {
      // Process the next prompt when this one is done
      _processNextPrompt();
    }).catchError((e) {
      // Still process next prompt even if there was an error
      _processNextPrompt();
    });
  }

  /// Wait for a prompt to complete
  ///
  /// [promptId] is the ID returned by sendPrompt
  Future<void> waitForCompletion(String promptId) async {
    if (!_promptCompleters.containsKey(promptId)) {
      // Prompt ID not found, might have already completed
      return;
    }

    final completer = _promptCompleters[promptId]!;
    await completer.future;
  }

  /// Stop text generation
  Future<void> stop() async {
    await _stopGeneration();
  }

  /// Create a new scope for this parent
  dynamic getScope() {
    // Import the scope class dynamically to avoid circular dependencies
    final scope = LlamaScope(this);
    _scopes.add(scope);
    return scope;
  }

  /// Dispose of resources
  Future<void> dispose() async {
    _isGenerating = false;
    _status = LlamaStatus.disposed;

    await _subscription?.cancel();

    // Dispose all scopes
    final scopesToDispose = List.from(_scopes);
    for (final scope in scopesToDispose) {
      await scope.dispose();
    }
    _scopes.clear();

    // Check if controller is already closed before trying to close it
    if (!_controller.isClosed) {
      await _controller.close();
    }

    if (!_completionController.isClosed) {
      await _completionController.close();
    }

    // Clear any pending operations
    for (final completer in _promptCompleters.values) {
      if (!completer.isCompleted) {
        completer.completeError(StateError("Parent disposed"));
      }
    }
    _promptCompleters.clear();

    // Clear any queued prompts with errors
    for (final queuedPrompt in _promptQueue) {
      if (!queuedPrompt.idCompleter.isCompleted) {
        queuedPrompt.idCompleter.completeError(StateError("Parent disposed"));
      }
    }
    _promptQueue.clear();

    // Send clear command to child before disposing
    _parent.sendToChild(id: 1, data: LlamaClear());

    // Allow time for clear command to be processed
    await Future.delayed(const Duration(milliseconds: 100));

    _parent.dispose();
  }

  /// Send a prompt with images to the model for VLM generation
  ///
  /// [prompt] The text prompt to generate from
  /// [images] List of images to include with the prompt
  /// [scope] Optional scope to associate with this prompt
  ///
  /// Returns a prompt ID that can be used to track completion
  Future<String> sendPromptWithImages(String prompt, List<LlamaImage> images,
      {Object? scope}) async {
    if (loadCommand.contextParams.embeddings) {
      throw StateError(
          "This LlamaParent instance is configured for embeddings only and cannot generate text.");
    }

    // Create a queued prompt with images
    final queuedPrompt = _QueuedPromptWithImages(prompt, images, scope);
    _promptQueue.add(queuedPrompt);

    // Start processing the queue if not already doing so
    if (!_isProcessingQueue) {
      _processNextPrompt();
    }

    // Return a Future that completes when this prompt gets an ID
    return queuedPrompt.idCompleter.future;
  }
}
