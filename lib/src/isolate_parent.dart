import 'dart:async';
import 'package:llama_cpp_dart/src/llama_input.dart';
import 'package:typed_isolate/typed_isolate.dart';

import 'isolate_child.dart';
import 'isolate_scope.dart';
import 'isolate_types.dart'; 
import 'llama.dart'; 
import 'prompt_format.dart';

/// Event emitted when a completion finishes or fails
class CompletionEvent {
  final String promptId;
  final bool success;
  final String? errorDetails;
  CompletionEvent(this.promptId, this.success, [this.errorDetails]);
}

/// Structure to hold queued prompts
class _QueuedPrompt {
  final String prompt;
  final Completer<String> idCompleter = Completer<String>();
  final Object? scope;
  final List<LlamaImage>? images;
  _QueuedPrompt(this.prompt, this.scope, {this.images});
}

/// Parent class that manages communication with the LlamaChild isolate
class LlamaParent {
  final StreamController<String> _controller = StreamController<String>.broadcast();
  final _parent = IsolateParent<LlamaCommand, LlamaResponse>();

  StreamSubscription<LlamaResponse>? _subscription;
  bool _isGenerating = false;
  LlamaStatus _status = LlamaStatus.uninitialized;
  LlamaStatus get status => _status;

  /// List of messages for chat history used by the formatter
  List<Map<String, dynamic>> messages = [];

  final LlamaLoad loadCommand;
  final PromptFormat? formatter;

  Completer<void>? _readyCompleter;
  Completer<void>? _operationCompleter;
  Completer<List<double>>? _embeddingsCompleter;
  
  /// Maps prompt IDs to completers for operation tracking
  final Map<String, Completer<void>> _promptCompleters = {};

  /// Stream of text generated by the model
  Stream<String> get stream => _controller.stream;
  
  /// Whether text generation is in progress
  bool get isGenerating => _isGenerating;

  final _completionController = StreamController<CompletionEvent>.broadcast();
  Stream<CompletionEvent> get completions => _completionController.stream;

  String _currentPromptId = "";
  final List<dynamic> _scopes = []; 
  final List<_QueuedPrompt> _promptQueue = [];
  bool _isProcessingQueue = false;

  Object? _currentScope;

  /// Create a new LlamaParent
  LlamaParent(this.loadCommand, [this.formatter]);

  /// Initialize the LlamaParent and load the model
  Future<void> init() async {
    _readyCompleter = Completer<void>();
    _isGenerating = false;
    _status = LlamaStatus.uninitialized;
    
    _parent.init();
    await _subscription?.cancel();
    _subscription = _parent.stream.listen(_onData);

    // Spawn the child isolate
    await _parent.spawn(LlamaChild());

    // Initialize library path
    await _sendCommand(
        LlamaInit(Llama.libraryPath),
        "library initialization");

    // Load model
    _status = LlamaStatus.loading;
    await _sendCommand(loadCommand, "model loading");

    await _readyCompleter!.future;
  }

  /// Handle responses from the child isolate
  void _onData(LlamaResponse data) {
    // 1. Handle Embeddings
    if (data.embeddings != null) {
      if (_embeddingsCompleter != null && !_embeddingsCompleter!.isCompleted) {
        _embeddingsCompleter!.complete(data.embeddings);
        _embeddingsCompleter = null;
      }
      return; 
    }

    if (data.status == LlamaStatus.error && data.errorDetails != null) {
      if (_operationCompleter != null && !_operationCompleter!.isCompleted) {
        _operationCompleter!.completeError(
          LlamaException(data.errorDetails!)
        );
        // Clear it so we don't try to complete it again later
        _operationCompleter = null; 
      }
    }

    // 2. Update Status
    if (data.status != null) {
      _status = data.status!;
      if (data.status == LlamaStatus.ready &&
          _readyCompleter != null &&
          !_readyCompleter!.isCompleted) {
        _readyCompleter!.complete();
      }
    }

    // 3. Handle Confirmation (ACKs for commands)
    if (data.isConfirmation) {
      if (_operationCompleter != null && !_operationCompleter!.isCompleted) {
        _operationCompleter!.complete();
      }
    }

    // 4. Handle Text Stream
    if (data.text.isNotEmpty) {
      _controller.add(data.text);
      for (final scope in _scopes) {
        scope.handleResponse(data);
      }
    }

    // 5. Handle Completion (End of generation or Error)
    if (data.isDone) {
      _isGenerating = false;
      final promptId = data.promptId ?? _currentPromptId;

      // Complete the specific prompt completer
      if (_promptCompleters.containsKey(promptId)) {
        if (!_promptCompleters[promptId]!.isCompleted) {
          _promptCompleters[promptId]!.complete();
        }
        _promptCompleters.remove(promptId);
      }

      final success = data.status != LlamaStatus.error;
      final event = CompletionEvent(promptId, success, data.errorDetails);
      
      _completionController.add(event);
      for (final scope in _scopes) {
        scope.handleCompletion(event);
      }
      _currentScope = null;
    }
  }

  /// Send a command to the child isolate and wait for confirmation
  Future<void> _sendCommand(LlamaCommand command, String description) async {
    _operationCompleter = Completer<void>();
    _parent.sendToChild(data: command, id: 1);
    
    return await _operationCompleter!.future.timeout(
      Duration(seconds: description == "model loading" ? 60 : 30),
      onTimeout: () {
        throw TimeoutException('Operation "$description" timed out');
      },
    );
  }

  /// Send a prompt to the model for text generation
  Future<String> sendPrompt(String prompt, {Object? scope}) async {
    if (loadCommand.contextParams.embeddings) {
      throw StateError("Configured for embeddings only.");
    }
    final queuedPrompt = _QueuedPrompt(prompt, scope);
    _promptQueue.add(queuedPrompt);
    if (!_isProcessingQueue) _processNextPrompt();
    return queuedPrompt.idCompleter.future;
  }
  
  /// Send a prompt with images to the model for VLM generation
  Future<String> sendPromptWithImages(String prompt, List<LlamaImage> images, {Object? scope}) async {
    if (loadCommand.contextParams.embeddings) {
      throw StateError("Configured for embeddings only.");
    }
    final queuedPrompt = _QueuedPrompt(prompt, scope, images: images);
    _promptQueue.add(queuedPrompt);
    if (!_isProcessingQueue) _processNextPrompt();
    return queuedPrompt.idCompleter.future;
  }

  /// Process the next prompt in the queue
  Future<void> _processNextPrompt() async {
    if (_promptQueue.isEmpty) {
      _isProcessingQueue = false;
      return;
    }

    _isProcessingQueue = true;
    final nextPrompt = _promptQueue.removeAt(0);

    // Ensure state is clean before starting new prompt
    if (_isGenerating) {
        await stop(); 
        // Small buffer to allow cleanup
        await Future.delayed(const Duration(milliseconds: 50));
    }

    _currentPromptId = DateTime.now().millisecondsSinceEpoch.toString();
    _promptCompleters[_currentPromptId] = Completer<void>();
    nextPrompt.idCompleter.complete(_currentPromptId);
    
    _currentScope = nextPrompt.scope;
    if (nextPrompt.scope != null) {
      (nextPrompt.scope as dynamic).addPromptId(_currentPromptId);
    }

    final formattedPrompt = messages.isEmpty 
        ? (formatter?.formatPrompt(nextPrompt.prompt) ?? nextPrompt.prompt)
        : (formatter?.formatMessages(messages) ?? nextPrompt.prompt);

    _isGenerating = true;
    _status = LlamaStatus.generating;

    _parent.sendToChild(
        id: 1,
        data: LlamaPrompt(formattedPrompt, _currentPromptId, images: nextPrompt.images));

    // Chain the queue
    _promptCompleters[_currentPromptId]!.future.whenComplete(() {
      _processNextPrompt();
    });
  }

  /// Wait for a prompt to complete
  /// [promptId] is the ID returned by sendPrompt
  Future<void> waitForCompletion(String promptId) async {
    if (!_promptCompleters.containsKey(promptId)) {
      // Prompt ID not found, might have already completed
      return;
    }

    final completer = _promptCompleters[promptId]!;
    await completer.future;
  }

  /// Stop text generation
  Future<void> stop() async {
    if (_isGenerating) {
      await _sendCommand(LlamaStop(), "generation stopping");
      _isGenerating = false;
    }
  }

  /// Get embeddings for a prompt
  Future<List<double>> getEmbeddings(String prompt) async {
    if (!loadCommand.contextParams.embeddings) {
      throw StateError("Not configured for embeddings.");
    }
    _embeddingsCompleter = Completer();
    _parent.sendToChild(id: 1, data: LlamaEmbedd(prompt));
    return _embeddingsCompleter!.future;
  }

  /// Dispose of resources
  Future<void> dispose() async {
    _isGenerating = false;
    _status = LlamaStatus.disposed;

    // Reject all pending completers
    if (_readyCompleter != null && !_readyCompleter!.isCompleted) _readyCompleter!.completeError("Disposed");
    if (_operationCompleter != null && !_operationCompleter!.isCompleted) _operationCompleter!.completeError("Disposed");
    if (_embeddingsCompleter != null && !_embeddingsCompleter!.isCompleted) _embeddingsCompleter!.completeError("Disposed");

    for(var p in _promptQueue) {
        if(!p.idCompleter.isCompleted) p.idCompleter.completeError("Disposed");
    }
    _promptQueue.clear();

    await _subscription?.cancel();
    
    for (final scope in List.from(_scopes)) {
      await scope.dispose(); 
    }
    _scopes.clear();

    if (!_controller.isClosed) await _controller.close();
    if (!_completionController.isClosed) await _completionController.close();

    _parent.sendToChild(id: 1, data: LlamaClear());
    // Give a tiny moment for the message to fly before killing the isolate
    await Future.delayed(const Duration(milliseconds: 10)); 
    _parent.dispose();
  }
  
  /// Create a new scope for this parent
  dynamic getScope() {
     final scope = LlamaScope(this);
     _scopes.add(scope);
     return scope;
  }
  
  /// Cancel prompts tied to a specific scope
  Future<void> cancelScope(Object scope, {bool cancelInFlight = true, bool cancelQueued = true}) async {
      if (cancelQueued) {
         _promptQueue.removeWhere((p) {
             if (identical(p.scope, scope)) {
                 if (!p.idCompleter.isCompleted) p.idCompleter.completeError("Scope canceled");
                 return true;
             }
             return false;
         });
      }
      if (cancelInFlight && identical(_currentScope, scope) && _isGenerating) {
          await stop();
      }
  }
}